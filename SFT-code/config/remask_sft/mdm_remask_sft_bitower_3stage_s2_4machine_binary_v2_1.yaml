desc: Remask-SFT

network_kwargs: 
    class_name: networks.llada.modeling_llada_bitowel.LLaDAUPMModelLM.from_pretrained
    pretrained_model_name_or_path: /path/to/GSAI-ML/LLaDA-8B-Instruct
    trust_remote_code: true
    torch_dtype: bfloat16

tokenizer_kwargs:
    class_name: transformers.AutoTokenizer.from_pretrained
    pretrained_model_name_or_path: /path/to/GSAI-ML/LLaDA-8B-Instruct

data_loader_kwargs: 
    class_name: dataloaders.dataloader_general_20250716_select_response_length_tokenized.load_multiple_dataset
    local_path: ["dataloaders/training_data.json"]
    num_replicas_per_dataset: [[0,1,2,3,4,5,6,7]]
    tokenizer_path: '/path/to/GSAI-ML/LLaDA-8B-Instruct'
    batch_size: 1
    max_length: 2048

loss_kwargs: 
    class_name: losses.loss_mask_weight_without_uniform_for_bitower_BD_sft_binary_v2_1.MaskWithoutUniformLoss
    mask_token_id: 126336
    block_size: 32
    upm_loss_type: all
    upm_loss_weight: 0.3

optimizer_kwargs:
    class_name: torch.optim.AdamW
    lr: 2.0e-6 # 5e-2/
    lr2: 2.0e-6 
    betas: [0.9, 0.999]
    eps: 1.0e-8
    weight_decay: 0 

lr_scheduler_kwargs:
    class_name: training.utils.lr_scheduler.cosine_with_warmup
    step_warmup: 50
    T_max: 20000
    eta_min: 0.5

# other training args
training_args:
    func_name: training.training_loop_fsdp_sft_bitower.training_loop
    run_dir: runs
    total_steps: 20000
    loss_scaling: 1
    grad_accumulation: 10
    max_grad_norm: 1 # default
    seed: 112
    step_per_tick: 10 # increase this
    snapshot_ticks: 100
    state_dump_ticks: 100
    precision: bf16
    val_ticks: 10
    skip_spike_grad: 1.0e+10
    start_step: 0
